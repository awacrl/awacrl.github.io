<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<link rel="StyleSheet" href="style.css" type="text/css" media="all" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>AWAC</title>
<style type="text/css">
#primarycontent h1 {
  font-variant: small-caps;
}
#primarycontent h3 {
}
#primarycontent teasertext {
  text-align: center;
}
#primarycontent p {
  text-align: center;
}
#primarycontent {
  text-align: justify;
}
#primarycontent p {
  text-align: justify;
  padding-left: 10px;
  padding-right: 10px;
}
#primarycontent p iframe {
  text-align: center;
}
.featart {
  margin:4px;
}
.hoverdiv {
  background-color:black;
  margin-top:2px;
  margin-bottom:10px;
  width:100%;
}
.hoverdiv:hover {
  background-color:white;
}
</style>

<script type="text/javascript">
  function togglevis(elid){
    el=document.getElementById(elid);
    aelid=elid+"a";
    ael=document.getElementById(aelid);
    if(el.style.display=='none'){
      el.style.display='inline-table';
      ael.innerHTML="[Hide BibTex]";
    }else{
      el.style.display='none';
      ael.innerHTML="[Show BibTex]";
    }
  }
</script>
<script type="text/javascript"
  src="http://www.maths.nottingham.ac.uk/personal/drw/LaTeXMathML.js">
</script>
<!--
<script type="text/javascript" src="http://math.etsu.edu/LaTeXMathML/LaTeXMathML.js"></script>
<link rel="stylesheet" type="text/css" href="http://math.etsu.edu/LaTeXMathML/LaTeXMathML.standardarticle.css" />
-->

<script>
   (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
   (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-93210824-1', 'auto');
    ga('send', 'pageview');
</script>

</head>
<body>
<div id="primarycontent">
<h1 align="center" itemprop="name"><strong>Accelerating Online Reinforcement Learning <br /> with Offline Datasets</strong></h1>

<center>
<ul id="people" itemprop="accountablePerson">
  <li><h4>
        <a href="https://ashvin.me/">Ashvin Nair</a>,
        <a href="https://mihdalal.github.io/">Murtaza Dalal</a>,
        <a href="https://people.eecs.berkeley.edu/~abhigupta/">Abhishek Gupta</a>,
        <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
  </h4></li>
  <li>
      <a href="http://arxiv.org/abs/2006.09359">paper</a> /
      <a href="https://github.com/vitchyr/rlkit/tree/master/examples/awac">code</a> /
      <!-- code / -->
      <a href="https://github.com/anair13/mj_envs">envs</a>
      <!-- data -->
  </li>
</ul>
</center>

<img src="fig1.png" itemprop="image" width="800" alt="teaserImage">

<h3 style="clear:both">Abstract</h3>
<p style="padding-left: 10px; padding-right: 10px;">
Reinforcement learning provides an appealing formalism for learning control policies from experience. However, the classic active formulation of reinforcement learning necessitates a lengthy active exploration process for each behavior, making it difficult to apply in real-world settings. If we can instead allow reinforcement learning to effectively use previously collected data to aid the online learning process, where the data could be expert demonstrations or more generally any prior experience, we could make reinforcement learning a substantially more practical tool for real-world control problems. While a number of recent methods have sought to learn offline from previously collected data, it remains exceptionally difficult to train a policy with offline data and improve it further with online reinforcement learning. In this paper we systematically analyze why this problem is so challenging, and propose a novel algorithm that combines sample-efficient dynamic programming with maximum likelihood policy updates, providing a simple and effective framework that is able to leverage large amounts of offline data and then quickly perform online fine-tuning of reinforcement learning policies. We show that our method enables rapid learning of skills with a combination of prior demonstration data and online experience across a suite of difficult dexterous manipulation and benchmark tasks.
</p>

<!--h3 style="clear:both">Preprint</h3>
<p style="padding-left: 10px; padding-right: 10px;">
  Preprint can be found <a href="https://arxiv.org/abs/1812.03201">here</a>.
</p><-->

<h3 style="clear:both">Video</h3>
<p style="padding-left: 10px; padding-right: 10px;">
  <center>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/XRP6H-vJGpo" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  </center>
</p>

<h3 style="clear:both">Environments</h3>
<p style="padding-left: 10px; padding-right: 10px;">
Environments are available at <a href="https://github.com/anair13/mj_envs">this repository</a>.
</p>

<h3 style="clear:both">Preprint</h3>
<p style="padding-left: 10px; padding-right: 10px;">
Preprint can be accessed on <a href="http://arxiv.org/abs/2006.09359">arXiv</a>.
</p>

<h3 style="clear:both">Website Template</h3>
<p style="padding-left: 10px; padding-right: 10px;">
The template for this website has been adopted from Carl Doersch.
</p>

<h3 style="clear:both">Contact</h3>
<p style="padding-left: 10px; padding-right: 10px;">
For comments/questions, contact <a href="http://ashvin.me">Ashvin Nair</a></p>
</div>

</body>
</html>
