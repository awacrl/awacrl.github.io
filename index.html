<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<link rel="StyleSheet" href="style.css" type="text/css" media="all" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<title>AWAC</title>
<style type="text/css">
#primarycontent h1 {
  font-variant: small-caps;
}
#primarycontent h3 {
}
#primarycontent teasertext {
  text-align: center;
}
#primarycontent p {
  text-align: center;
}
#primarycontent {
  text-align: justify;
}
#primarycontent p {
  text-align: justify;
  padding-left: 10px;
  padding-right: 10px;
}
#primarycontent p iframe {
  text-align: center;
}
.featart {
  margin:4px;
}
.hoverdiv {
  background-color:black;
  margin-top:2px;
  margin-bottom:10px;
  width:100%;
}
.hoverdiv:hover {
  background-color:white;
}
</style>

<script type="text/javascript">
  function togglevis(elid){
    el=document.getElementById(elid);
    aelid=elid+"a";
    ael=document.getElementById(aelid);
    if(el.style.display=='none'){
      el.style.display='inline-table';
      ael.innerHTML="[Hide BibTex]";
    }else{
      el.style.display='none';
      ael.innerHTML="[Show BibTex]";
    }
  }
</script>
<script type="text/javascript"
  src="http://www.maths.nottingham.ac.uk/personal/drw/LaTeXMathML.js">
</script>
<!--
<script type="text/javascript" src="http://math.etsu.edu/LaTeXMathML/LaTeXMathML.js"></script>
<link rel="stylesheet" type="text/css" href="http://math.etsu.edu/LaTeXMathML/LaTeXMathML.standardarticle.css" />
-->

<script>
   (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
   (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-93210824-1', 'auto');
    ga('send', 'pageview');
</script>

</head>
<body>
<div id="primarycontent">
<h1 align="center" itemprop="name"><strong>AWAC: Accelerating Online Reinforcement Learning <br /> with Offline Datasets</strong></h1>

<center>
<ul id="people" itemprop="accountablePerson">
  <li><h4>
        <a href="https://ashvin.me/">Ashvin Nair*</a>,
        <a href="https://people.eecs.berkeley.edu/~abhigupta/">Abhishek Gupta*</a>,
        <a href="https://mihdalal.github.io/">Murtaza Dalal</a>,
        <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
  </h4></li>
  <li>
      <a href="http://arxiv.org/abs/2006.09359">paper</a> /
      <a href="https://github.com/vitchyr/rlkit/tree/master/examples/awac">code</a> /
      <!-- code / -->
      <a href="https://github.com/anair13/mj_envs">envs</a>
      <!-- data -->
  </li>
</ul>
</center>

<img src="fig1.png" itemprop="image" width="800" alt="teaserImage">

<h3 style="clear:both">Abstract</h3>
<p style="padding-left: 10px; padding-right: 10px;">
Reinforcement learning (RL) provides an appealing formalism for learning control policies from experience. However, the classic active formulation of RL necessitates a lengthy active exploration process for each behavior, making it difficult to apply in real-world settings such as robotic control. If we can instead allow RL algorithms to effectively use previously collected data to aid the online learning process, such applications could be made substantially more practical: the prior data would provide a starting point that mitigates challenges due to exploration and sample complexity, while the online training enables the agent to perfect the desired skill. Such prior data could either constitute expert demonstrations or, more generally, sub-optimal prior data that illustrates potentially useful transitions. While a number of prior methods have either used optimal demonstrations to bootstrap reinforcement learning, or have used sub-optimal data to train purely offline, it remains exceptionally difficult to train a policy with potentially sub-optimal offline data and actually continue to improve it further with online RL. In this paper we systematically analyze why this problem is so challenging, and propose an algorithm that combines sample-efficient dynamic programming with maximum likelihood policy updates, providing a simple and effective framework that is able to leverage large amounts of offline data and then quickly perform online fine-tuning of RL policies. We show that our method, advantage weighted actor critic (AWAC), enables rapid learning of skills with a combination of prior demonstration data and online experience. We demonstrate these benefits on a variety of simulated and real-world robotics domains, including dexterous manipulation with a real multi-fingered hand, drawer opening with a robotic arm, and rotating a valve. Our results show that incorporating prior data can reduce the time required to learn a range of robotic skills to practical time-scales.
</p>

<!--h3 style="clear:both">Preprint</h3>
<p style="padding-left: 10px; padding-right: 10px;">
  Preprint can be found <a href="https://arxiv.org/abs/1812.03201">here</a>.
</p><-->

<h3 style="clear:both">Video</h3>
<p style="padding-left: 10px; padding-right: 10px;">
  <center>
    <iframe width="560" height="315" src="https://www.youtube.com/embed/gquat2_NLwQ" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  </center>
</p>

<h3 style="clear:both">Results</h3>
<p style="padding-left: 10px; padding-right: 10px;">
  <center>
    <table style="width:100%">
      <tr>
        <th>Simulation Results</th>
        <th>Real World Results</th>
      </tr>
      <tr>
        <td>
          <center>Pen Repositioning</center>
          <img src="gifs/sim_pen.gif">
        </td>
        <td>
          <center>D'Claw</center>
          <img src="gifs/real_claw.gif">
        </td>
      </tr>
      <tr>
        <td>
          <center>Door Opening</center>
          <img src="gifs/sim_door.gif">
        </td>
        <td>
          <center>Drawer Opening</center>
          <img src="gifs/real_drawer.gif">
        </td>
      </tr>
      <tr>
        <td>
          <center>Object Relocation</center>
          <img src="gifs/sim_relocate.gif">
        </td>
        <td>
          <center>D'Hand</center>
          <img src="gifs/real_hand.gif">
        </td>
      </tr>
    </table>
  </center>
</p>

<h3 style="clear:both">Environments</h3>
<p style="padding-left: 10px; padding-right: 10px;">
Environments are available at <a href="https://github.com/anair13/mj_envs">this repository</a>.
</p>

<h3 style="clear:both">Preprint</h3>
<p style="padding-left: 10px; padding-right: 10px;">
Preprint can be accessed on <a href="http://arxiv.org/abs/2006.09359">arXiv</a>.
</p>

<h3 style="clear:both">Website Template</h3>
<p style="padding-left: 10px; padding-right: 10px;">
The template for this website has been adopted from Carl Doersch.
</p>

<h3 style="clear:both">Contact</h3>
<p style="padding-left: 10px; padding-right: 10px;">
For comments/questions, contact <a href="http://ashvin.me">Ashvin Nair</a></p>
</div>

</body>
</html>
